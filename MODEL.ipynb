{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MODEL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-HQS40STUvF"
      },
      "source": [
        "# THIS FUNCTION TRAIN ON DIFFERENT MODELS, GET THE SCORE OF EACH MODELS AND CREATE A DICTIONARY OF MODELS AND RESPECTIVE SCORES.\n",
        "\n",
        "\n",
        "def model_train(df_X,df_Y, metric):\n",
        "    !pip install lightgbm\n",
        "    !pip install catboost\n",
        "    !pip install xgboost\n",
        "    !pip install rgf_python\n",
        "    Models=dict()\n",
        "    from sklearn.metrics import metric \n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from catboost import CatBoostClassifier\n",
        "    from lightgbm import LGBMClassifier\n",
        "    from xgboost import XGBClassifier\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    from rgf.sklearn import RGFClassifier\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    x_train, x_test, y_train, y_test= train_test_split(df_X, df_Y, test_size=0.2, random_state=42)\n",
        "    catboost=CatBoostClassifier()\n",
        "    lightgbm=LGBMClassifier()\n",
        "    xgb=XGBClassifier()\n",
        "    Lr=LogisticRegression()\n",
        "    Rf=RandomForestClassifier()\n",
        "    Gb=GradientBoostingClassifier()\n",
        "    rgf=RGFClassifier()\n",
        "    mlp=MLPClassifier()\n",
        "    Model_list=[catboost, lightgbm, xgb, Lr, Rf, Gb, rgf, mlp]\n",
        "    for model in Model_list:\n",
        "        len_of_model=len(Model_list)\n",
        "        current=(Model_list.index(model)) + 1\n",
        "        print (\"Training model \" + str(current) +  \" of \" + str(len_of_model)  )\n",
        "        model.fit(x_train, y_train)\n",
        "        y_pred=model.predict(x_test)\n",
        "        sc=metric(y_test, y_pred)\n",
        "        Models[model]=sc\n",
        "    print (Models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVVyqyWZ-bVb"
      },
      "source": [
        "Best models from model_train function goes into hyperopt function in list format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zwdyTqM-W7n"
      },
      "source": [
        "!pip install hyperopt\n",
        "from hyperopt import tpe, hp, fmin, STATUS_OK,Trials\n",
        "from hyperopt.pyll.base import scope\n",
        "\n",
        "def optimization(Models, X, y):\n",
        "    for model in Models:\n",
        "        len_of_model=len(Models)\n",
        "        current=(Models.index(model)) + 1\n",
        "        print (\"Running hyperopt on \" + str(current) +  \" of \" + str(len_of_model)  )\n",
        "        def hyperopt(params):\n",
        "            clf = model(**params)\n",
        "            cross_val_score(clf, X, y, cv=3).mean()\n",
        "\n",
        "        space = {\n",
        "         \"n_estimators\": hp.choice(\"n_estimators\", [100, 200, 300, 400,500,600]),\n",
        "          \"max_depth\": hp.quniform(\"max_depth\", 1, 15,1),\n",
        "            }\n",
        "\n",
        "        def f(params):\n",
        "            acc = hyperopt(params)\n",
        "            return {'loss': -acc, 'status': STATUS_OK}\n",
        "\n",
        "        trials = Trials()\n",
        "        best = fmin(f, space, algo=tpe.suggest, max_evals=3, trials=trials)\n",
        "        print ('best:')\n",
        "        print (best)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}